{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41df638f",
   "metadata": {},
   "source": [
    "## 얼굴 남기기 crop 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4457888d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'기쁨'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "face_cascade = cv2.CascadeClassifier('./haarcascade_frontface.xml')\n",
    "\n",
    "\n",
    "# 소스 데이터 path및 경로명\n",
    "src_path = '.\\\\2020-02-092.한국인감정인식_sample\\\\원천데이터'\n",
    "\n",
    "\n",
    "folder_list = os.listdir(src_path)\n",
    "folder = folder_list[0]\n",
    "folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "393bb742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장할 폴더 제작\n",
    "dest_path = './emotion_cv2'\n",
    "try:\n",
    "    os.mkdir(dest_path)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70db026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장할 폴더 제작\n",
    "for f in folder_list:\n",
    "    try:\n",
    "        os.mkdir(os.path.join(dest_path , f))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7a7ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████████████████▏                                                          | 154/580 [00:04<00:11, 38.24it/s]"
     ]
    }
   ],
   "source": [
    "for cnt, f in enumerate(folder_list):\n",
    "    src_f = os.path.join(src_path, f)\n",
    "    dest_f = os.path.join(dest_path, f)\n",
    "    image_list = os.listdir(src_f)\n",
    "    for i in tqdm(image_list):\n",
    "        src_image = os.path.join(src_f, i)\n",
    "        dest_image = os.path.join(dest_f, i)\n",
    "        img_array = np.fromfile(src_image, np.uint8)\n",
    "        img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
    "\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, minNeighbors=4, minSize = [256,256])\n",
    "        if len(faces) == 1:\n",
    "            for (x,y,w,h) in faces:\n",
    "                cropped = img[y-60:y+h+60, x-30:x+w+30]\n",
    "                extension = os.path.splitext(i)[1] # 이미지 확장자\n",
    "                try:\n",
    "                    result, encoded_img = cv2.imencode(extension, cropped)\n",
    "                    if result:\n",
    "                        with open(dest_image, mode='w+b') as f:\n",
    "                            encoded_img.tofile(f)\n",
    "                except:\n",
    "                    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61631075",
   "metadata": {},
   "source": [
    "## 중복제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77003982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 폴더에 합쳐주기\n",
    "src_path = './emotion_cv2'\n",
    "dest_f = './emotion_cv2/총합'\n",
    "try:\n",
    "    os.mkdir(folder_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import shutil\n",
    "\n",
    "for f in folder_list:\n",
    "    src_f = os.path.join(src_path, f)\n",
    "    image_list = os.listdir(src_f)\n",
    "    for i in tqdm(image_list):\n",
    "        src_image = os.path.join(src_f, i)\n",
    "        shutil.move(src_image,dest_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39349e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = os.listdir(dest_f)\n",
    "image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd919a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복제거\n",
    "from PIL import Image\n",
    "\n",
    "for i in tqdm(image_list):\n",
    "    i_list = i.split('_')\n",
    "    cnt = 0\n",
    "    removed_images = []\n",
    "    for i2 in image_list:\n",
    "        i2_list = i2.split('_')\n",
    "        # 사진 파일이 같을 경우\n",
    "        if (i_list[0] == i2_list[0]) & (i2[-11:] == i[-11:]) :\n",
    "            cnt += 1\n",
    "            # 자기 자신이 아닐 경우\n",
    "            if cnt > 1 :\n",
    "                os.remove(os.path.join(dest_f,i2))\n",
    "                removed_images.append(i2)\n",
    "                \n",
    "    for r in removed_images:\n",
    "        image_list.remove(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f429402a",
   "metadata": {},
   "source": [
    "## 검수자 의견 반영"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cf5de2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# json 파일 불러오기 (전부 불러오기\n",
    "root = '.\\\\2020-02-092.한국인감정인식_sample\\\\라벨링데이터\\\\' # 절대경로임.\n",
    "classes = ['기쁨', '당황', '분노', '불안', '상처','슬픔','중립']\n",
    "\n",
    "for class_name in classes:\n",
    "    with open( root + class_name +'\\\\img_emotion_sample_data' + '(' + class_name + ')' +'.json') as f:\n",
    "        globals()['감정_{}'.format(class_name)] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4eef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "감정_기쁨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6b12cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#json 파일별 데이터프레임 생성\n",
    "df_delight =pd.DataFrame(감정_기쁨)\n",
    "df_embarrassing = pd.DataFrame(감정_당황)\n",
    "df_anger = pd.DataFrame(감정_분노)\n",
    "df_anxious = pd.DataFrame(감정_불안)\n",
    "df_hurt = pd.DataFrame(감정_상처)\n",
    "df_sad = pd.DataFrame(감정_슬픔)\n",
    "df_neutral =pd.DataFrame(감정_중립)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c26ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임들 연결\n",
    "df_list = [df_delight,df_embarrassing,df_anger,df_anxious,df_hurt,df_sad,df_neutral]\n",
    "df_all = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cc5eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout\n",
    "columns = df_delight.columns\n",
    "drop_columns =['gender', 'age', 'isProf','bg_uploader']\n",
    "df_all.drop(drop_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5359c290",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['annot_A'] = df_all['annot_A'].apply(lambda x : x['faceExp'])\n",
    "df_all['annot_B'] = df_all['annot_B'].apply(lambda x : x['faceExp'])\n",
    "df_all['annot_C'] = df_all['annot_C'].apply(lambda x : x['faceExp'])\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad33aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이름 파싱\n",
    "df_all['filename'] = df_all['filename'].apply(lambda x: x[:65]+x[-11:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7e763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_df_all = pd.get_dummies(df_all, columns=['annot_A','annot_B','annot_C'], prefix='')\n",
    "dm_df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a85edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in classes:\n",
    "    df_all[i] = dm_df_all[f'_{i}'].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5387be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_all = df_all.copy()\n",
    "final_df_all = final_df_all.drop(['faceExp_uploader','annot_A','annot_B','annot_C'], axis=1)\n",
    "final_df_all = final_df_all.groupby('filename').sum()\n",
    "final_df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6577ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_all[final_df_all.index == '2b69d97fbf9eab1da334021e7dd1352903907c88f84640d859cbdce1263ced5c_002-001.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d66bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_list = final_df_all.max(axis=1)\n",
    "vote_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf2b93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# 경로 설정\n",
    "save_dir = '.\\\\emotion_cv2\\\\final_data'\n",
    "image_list = glob('emotion_cv2\\\\총합\\\\*.jpg')\n",
    "\n",
    "\n",
    "# 최종 결과물 저장할 폴더 제작\n",
    "try:\n",
    "    os.mkdir(save_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.mkdir('.\\\\emotion_cv2\\\\final_data\\\\재분류 필요')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 저장할 폴더 제작\n",
    "for f in folder_list:\n",
    "    try:\n",
    "        os.mkdir(os.path.join(save_dir , f))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "for img in tqdm(image_list):\n",
    "    # 이미지 있는지 체크\n",
    "    \n",
    "    if os.path.isfile(img):\n",
    "        # max값 찾아서 index에 저장하고 몇번 max인지 횟수도 저장\n",
    "        # 62-15 = 47\n",
    "        file_name = img[15:]\n",
    "        df_file_name = str(file_name[:65] + file_name[-11:])\n",
    "        row = final_df_all[final_df_all.index == df_file_name]\n",
    "        row_list = list(row.values[0])\n",
    "        \n",
    "        max_index = -1\n",
    "        max_index_count = 0\n",
    "        for cnt2, i in enumerate(row_list):\n",
    "            max_vote = vote_list[vote_list.index== df_file_name].values[0]\n",
    "            if max_vote == i:\n",
    "                max_index = cnt2\n",
    "                max_index_count += 1\n",
    "\n",
    "        ## 최종적으로 이미지 복사\n",
    "        if max_index_count == 1:\n",
    "            img_folder_name = classes[max_index]\n",
    "            shutil.copy(img, os.path.join(save_dir ,img_folder_name ))\n",
    "        else :\n",
    "            shutil.copy(img, os.path.join(save_dir , \"재분류 필요\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbe28df",
   "metadata": {},
   "source": [
    "## 폴더 합치기 당황 + 불안, 슬픔 + 상처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6008c284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 당황 + 불안\n",
    "\n",
    "f_path = os.path.join(save_dir, '불안')\n",
    "image_list = os.listdir(f_path)\n",
    "for i in image_list:\n",
    "    i_path = os.path.join(f_path, i)\n",
    "    shutil.move(i_path, os.path.join(save_dir, '당황'))\n",
    "os.rmdir(f_path)\n",
    "    \n",
    "f_path = os.path.join(save_dir, '상처')\n",
    "image_list = os.listdir(f_path)\n",
    "for i in image_list:\n",
    "    i_path = os.path.join(f_path, i)\n",
    "    shutil.move(i_path, os.path.join(save_dir, '슬픔'))\n",
    "os.rmdir(f_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a52af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 재분류 폴더 및 파일 삭제, 필요시 제외시켜줄 수 있음\n",
    "shutil.rmtree('.\\\\emotion_cv2\\\\final_data\\\\재분류 필요')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7823fb6d",
   "metadata": {},
   "source": [
    "# 모델 설계 및 분석 파이토치로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4deb222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 전 캐시 지우기\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 디버깅용\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6951bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# dataset폴더의 데이터 불러오기\n",
    "# classes_list에 폴더명으로 클래스 넣기\n",
    "# os.listdir은 경로 하위의 모든 파일 및 폴더 목록을 리스트로 가져오는 메서드\n",
    "original_dataset_dir = '.\\\\emotion_cv2\\\\final_data'   \n",
    "classes_list = os.listdir(original_dataset_dir) \n",
    "\n",
    "# base_dir만들기\n",
    "base_dir = './splitted2' \n",
    "try:\n",
    "    os.mkdir(base_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# base폴더 안에 train, val, test폴더 만들기\n",
    "train_dir = os.path.join(base_dir, 'train') \n",
    "validation_dir = os.path.join(base_dir, 'val')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "try:\n",
    "    os.mkdir(train_dir)\n",
    "    os.mkdir(validation_dir)\n",
    "    os.mkdir(test_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 클래스 이름 리스트에 해당하는 폴더들을 각 폴더 안에 제작\n",
    "for cls in classes_list:\n",
    "    try:\n",
    "        os.mkdir(os.path.join(train_dir, cls))\n",
    "        os.mkdir(os.path.join(validation_dir, cls))\n",
    "        os.mkdir(os.path.join(test_dir, cls))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d20a6b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size( 기쁨 ):  51\n",
      "Validation size( 기쁨 ):  17\n",
      "Test size( 기쁨 ):  17\n",
      "Train size( 당황 ):  90\n",
      "Validation size( 당황 ):  30\n",
      "Test size( 당황 ):  30\n",
      "Train size( 분노 ):  55\n",
      "Validation size( 분노 ):  18\n",
      "Test size( 분노 ):  18\n",
      "Train size( 슬픔 ):  116\n",
      "Validation size( 슬픔 ):  38\n",
      "Test size( 슬픔 ):  38\n",
      "Train size( 중립 ):  151\n",
      "Validation size( 중립 ):  50\n",
      "Test size( 중립 ):  50\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "    \n",
    "for cls in classes_list:\n",
    "    \n",
    "    # 오리지널 dataset에 클래스 이름별로 접근해 해당 파일들의 이름을 fnames에 저장\n",
    "    path = os.path.join(original_dataset_dir, cls)\n",
    "    fnames = os.listdir(path)\n",
    "    \n",
    "    # fnames의 크기에 따라 train, val, test size를 6:2:2로 결정\n",
    "    train_size = math.floor(len(fnames) * 0.6)\n",
    "    validation_size = math.floor(len(fnames) * 0.2)\n",
    "    test_size = math.floor(len(fnames) * 0.2)\n",
    "    \n",
    "    # train 사이즈 만큼 f_names를 잘라서 train_fnames로 만들고 이를 해당 폴더에 복사\n",
    "    train_fnames = fnames[:train_size]\n",
    "    print(\"Train size(\",cls,\"): \", len(train_fnames))\n",
    "    for fname in train_fnames:\n",
    "        src = os.path.join(path, fname)\n",
    "        dst = os.path.join(os.path.join(train_dir, cls), fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "    \n",
    "    # val 사이즈 만큼 f_names를 잘라서 train_fnames로 만들고 이를 해당 폴더에 복사\n",
    "    validation_fnames = fnames[train_size:(validation_size + train_size)]\n",
    "    print(\"Validation size(\",cls,\"): \", len(validation_fnames))\n",
    "    for fname in validation_fnames:\n",
    "        src = os.path.join(path, fname)\n",
    "        dst = os.path.join(os.path.join(validation_dir, cls), fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "    # test 사이즈 만큼 f_names를 잘라서 train_fnames로 만들고 이를 해당 폴더에 복사\n",
    "    test_fnames = fnames[(train_size+validation_size):(validation_size + train_size +test_size)]\n",
    "    print(\"Test size(\",cls,\"): \", len(test_fnames))\n",
    "    for fname in test_fnames:\n",
    "        src = os.path.join(path, fname)\n",
    "        dst = os.path.join(os.path.join(test_dir, cls), fname)\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "100a88e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 베이스 라인 모델 학습\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# CUDA사용가능 여부 확인 및 가능할시 device를 cuda로 설정\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "\n",
    "# 학습 및 평가 과정에서 배치사이즈를 여러번 입력해야 함.\n",
    "# 배치 사이즈, 에포크를 변경하고 싶을 때 모든 경우를 다 찾아서 바꿔주기 어려우므로 변수에 저장해서 사용\n",
    "BATCH_SIZE = 24\n",
    "EPOCH = 10\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db90eabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder \n",
    "\n",
    "# transforms.Compose는 이미지 전처리 및 증강(augumentation)에서 사용.\n",
    "# 좌우 반전 밝기 조절 임의 확대 등이 있음.\n",
    "# .Resize는 이미지 크기 조절, ToTensor는 이미지를 텐서의 형태로 변환하고 모든 값을 0에서 1사이로 정규화함.\n",
    "transform_base = transforms.Compose([transforms.Resize((64,64)), transforms.ToTensor()])\n",
    "\n",
    "# ImageFolder메서드는 데이터셋을 불러오는 메서드. 하나의 클래스가 하나의 폴더에 자동으로 대응됨. \n",
    "# root옵션에 데이터를 불러올 경로 설정, transform 옵션에는 데이터를 불러온 후 전처리 또는 증강할 방법을 지정\n",
    "# 현재는 위에서 .Compose메서드로 만들어놓은 것을 사용\n",
    "train_dataset = ImageFolder(root='./splitted2/train', transform=transform_base) \n",
    "val_dataset = ImageFolder(root='./splitted2/val', transform=transform_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f0f8dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Dataloader는 불러온 이미지 데이터를 주어진 조건에 따라 미니 배치 단위로 분리하는 역할을 수행.\n",
    "# 학습 과정에 사용될 Dataloader는 train_dataset을 이용하여 생성\n",
    "# shuffle 트루 옵션으로 데이터의 순서를 섞어줄 수 있어 Label이 정보의 순서를 기억하는 것을 방지\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0019118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이스 라인 모델 설계\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    " \n",
    "# 딥러닝 모델과 관련된 기본적인 함수를 포함하는 nn.Module클래스 상속    \n",
    "# 상속을 하였으므로 nn.Module안의 여러 메서드를 사용 가능.\n",
    "class Net(nn.Module): \n",
    "    \n",
    "    # 모델에서 사용할 모든 레이어를 정의\n",
    "    def __init__(self):     \n",
    "    \n",
    "        super(Net, self).__init__()     # nn.Module내에 있는 메서드를 상속\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)   # Conv2d 레이어 정의 처음 3개 파라미터는 입력채널 수, 출력 채널 수, 커널 크기\n",
    "        self.pool = nn.MaxPool2d(2,2)                 # 맥스풀링. 파라미터는 커널 크기, stride의미\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)  # 위와 비슷하게 컨볼루션 레이어 설정\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)  \n",
    "\n",
    "        # Flatten이후 사용될 첫 번째 Fully Connected Layer정의. Flatten이후 사용되므로 Layer입력 채널 수는 x.view()의 출력 채널 수\n",
    "        self.fc1 = nn.Linear(64*64, 512)\n",
    "        self.fc2 = nn.Linear(512, 5)             # 두번째 완전연결레이어 정의 모델의 마지막 레이어라 클래스의 수만큼 출력\n",
    "    \n",
    "    \n",
    "    # 클래스 내부의 forward함수는 모델이 학습 데이터를 입력받아 Forward Propagation을 실행시켜 Output계산하는 과정\n",
    "    def forward(self, x):  \n",
    "        \n",
    "        # init의 self.으로 만든 이름들을 이용해서 forward진행.\n",
    "        #  컨볼루션 레이어로 featuremap생성\n",
    "        x = self.conv1(x)\n",
    "        # 비선형 활성화 함수 렐루 껴넣음\n",
    "        x = F.relu(x)  \n",
    "        \n",
    "        # 풀링 적용\n",
    "        x = self.pool(x) \n",
    "        # p는 비율, 0.25는 25프로의 노드를 드롭아웃 하겠다는 의미, training = self.training은 학습 모드와 검증 모드 구별을 위해 사용\n",
    "        # 학습 모드일 때는 드롭아웃을 쓰지만 평과 과정에서는 모든 노드를 사용해야하므로\n",
    "        x = F.dropout(x, p=0.25, training=self.training) \n",
    "\n",
    "        # 위와 마찬가지로 흘러감\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x) \n",
    "        x = self.pool(x) \n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "\n",
    "        x = self.conv3(x) \n",
    "        x = F.relu(x) \n",
    "        x = self.pool(x) \n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "\n",
    "        # 생성된 featrue Map을 1차원으로 펼치는 과정인 Flatten수행\n",
    "        x = x.view(-1, 64*64)  \n",
    "        # flatten된 tensor를 리니어 레이어에 통과시킴\n",
    "        x = self.fc1(x) \n",
    "        x = F.relu(x) \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        # 마찬가지\n",
    "        x = self.fc2(x) \n",
    "\n",
    "        # soft_max함수를 이용해서 각 클래스에 속할 확률을 Output값으로 출력\n",
    "        return F.log_softmax(x, dim=1)  \n",
    "\n",
    "# 모델을 현재 사용중인 장비에 할당하며 생성\n",
    "model_base = Net().to(DEVICE)  \n",
    "optimizer = optim.Adam(model_base.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8146bccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 함수 모델, 트레인로더, 옵티마이저 받음\n",
    "from tqdm import tqdm\n",
    "def train(model, train_loader, optimizer):\n",
    "    model.train()    # 입력받는 모델을 학습 모드로 설정\n",
    "    # train_loader에는 (data, target) 형태가 미니 배치 단위로 묶여 있음.\n",
    "    # enumerate를 적용하여 아래와 같은 형태로 for문 실행\n",
    "    for batch_idx, (data, target) in tqdm(enumerate(train_loader)):\n",
    "        \n",
    "        # 사용중인 장비에 해당 data와 target변수를 할당\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE) \n",
    "        \n",
    "        # 이전 Batch의 Gradient값이 optimizer에 저장되어 있으므로 optimizer를 초기화\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # 모델에 data를 넣어 output값을 계산\n",
    "        output = model(data)  \n",
    "        # output값과 target값 사이의 loss를 계산.\n",
    "        # 분류문제에는 Cross Entropy Loss 사용\n",
    "        loss = F.cross_entropy(output, target) \n",
    "        \n",
    "        # loss값을 바탕으로 BackPropagation을 통해 계산한 Gradient값을 각 Parameter에 할당\n",
    "        loss.backward()\n",
    "        # 각 파라미터에 할당된 Gradient값을 이용해 모델의 Parameter를 업데이트 함\n",
    "        optimizer.step()  \n",
    "#         torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36fba50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가를 위한 함수\n",
    "def evaluate(model, test_loader):\n",
    "    # 입력받은 모델을 평가모드로 설정\n",
    "    model.eval()  \n",
    "    # 미니 배치별로 Loss를 합산해서 저장할 변수인 test_loss를 선언\n",
    "    test_loss = 0 \n",
    "    # 올바르게 예측한 데이터의 수를 세는 변수인 correct선언\n",
    "    correct = 0   \n",
    "    \n",
    "    with torch.no_grad():        # 모델을 평가할 때에는 Parameter를 업데이트 하지 않아야 함, torch.no_grad()메서드로 업데이트 중단\n",
    "        \n",
    "        for data, target in test_loader:  \n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)  \n",
    "            output = model(data) \n",
    "            \n",
    "            # 로스를 계산해서 해당 로스 값을 .item()을 test_loss에 더해줌\n",
    "            test_loss += F.cross_entropy(output,target, reduction='sum').item() \n",
    " \n",
    "            # 모델에 입력된 test데이터가 33개의 클래스에 속할 확률값이 각각 output으로 출력됨. 따라서 가장 높은 값 추출 pred에 저장\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            # 만약 target.view_as(pred)를 통해 target텐서 구조를 pred 텐서와 같은 모양으로 정렬\n",
    "            \n",
    "            # view_as 메서드는 적용 대상 Tensor를 메서드에 입력되는 Tensor의 모양대로 재정렬함.\n",
    "            # 앞서 모델 구조를 구성할 때(flatten) 학습했던 view()함수는 정렬하고 싶은 Tensor모양을 숫자로 직접지정한다는 것에서 차이\n",
    "            # eq 메서드는 객체 간의 비교 연산자 일치하면 1, 일치하지 않으면 0을 반환\n",
    "            # .sum부터는 왜 붙이지..?\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "#             torch.cuda.empty_cache()\n",
    "    \n",
    "    # 모든 미니 Batch합한 loss값을 batch수로 나누어 미니 배치마다 계산된 loss값의 평균을 구함\n",
    "    test_loss /= len(test_loader.dataset) \n",
    "    \n",
    "    # 정확도 평균 구하기\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset) \n",
    "    return test_loss, test_accuracy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3c8bef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:02,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- epoch 1 ----------------\n",
      "train Loss: 1.5483, Accuracy: 32.61%\n",
      "val Loss: 1.5480, Accuracy: 32.68%\n",
      "Completed in 0m 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 22.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- epoch 2 ----------------\n",
      "train Loss: 1.5485, Accuracy: 32.61%\n",
      "val Loss: 1.5526, Accuracy: 32.68%\n",
      "Completed in 0m 9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 23.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- epoch 3 ----------------\n",
      "train Loss: 1.5266, Accuracy: 32.61%\n",
      "val Loss: 1.5388, Accuracy: 32.68%\n",
      "Completed in 0m 9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 22.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- epoch 4 ----------------\n",
      "train Loss: 1.5335, Accuracy: 35.64%\n",
      "val Loss: 1.5694, Accuracy: 30.72%\n",
      "Completed in 0m 9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 24.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- epoch 5 ----------------\n",
      "train Loss: 1.4502, Accuracy: 36.07%\n",
      "val Loss: 1.5523, Accuracy: 34.64%\n",
      "Completed in 0m 9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 25.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- epoch 6 ----------------\n",
      "train Loss: 1.3514, Accuracy: 49.24%\n",
      "val Loss: 1.6088, Accuracy: 14.38%\n",
      "Completed in 0m 9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 23.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- epoch 7 ----------------\n",
      "train Loss: 1.2737, Accuracy: 47.73%\n",
      "val Loss: 1.6681, Accuracy: 18.95%\n",
      "Completed in 0m 9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 22.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- epoch 8 ----------------\n",
      "train Loss: 1.1713, Accuracy: 54.43%\n",
      "val Loss: 1.6912, Accuracy: 21.57%\n",
      "Completed in 0m 9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 22.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- epoch 9 ----------------\n",
      "train Loss: 1.1119, Accuracy: 55.94%\n",
      "val Loss: 1.7160, Accuracy: 22.22%\n",
      "Completed in 0m 9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 22.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- epoch 10 ----------------\n",
      "train Loss: 1.0669, Accuracy: 58.53%\n",
      "val Loss: 1.7321, Accuracy: 26.14%\n",
      "Completed in 0m 10s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "# \n",
    "def train_baseline(model ,train_loader, val_loader, optimizer, num_epochs = 30):\n",
    "    \n",
    "    # 정확도가 가장 높은 모델의 정확도를 저장하는 변수\n",
    "    best_acc = 0.0  \n",
    "    \n",
    "    # 정확도가 가장 높은 모델을 저장할 변수\n",
    "    best_model_wts = copy.deepcopy(model.state_dict()) \n",
    " \n",
    "    # 에포크 단위로 돌아감\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # 소요시간 측정용\n",
    "        since = time.time()\n",
    "        \n",
    "        # train함수와 evaluate함수 이용하여 train, val의 loss와 정확도 계산\n",
    "        train(model, train_loader, optimizer)\n",
    "        train_loss, train_acc = evaluate(model, train_loader) \n",
    "        val_loss, val_acc = evaluate(model, val_loader)\n",
    "        \n",
    "        # 현재 epoch의 검증 정확도가 최고 정확도 보다 높다면 best_acc를 현재 epoch의 검증 정확도로 업데이트 하고 모델 저장\n",
    "        if val_acc > best_acc: \n",
    "            best_acc = val_acc \n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        # 한에포크당 소요시간 계산\n",
    "        time_elapsed = time.time() - since \n",
    "        print('-------------- epoch {} ----------------'.format(epoch))\n",
    "        print('train Loss: {:.4f}, Accuracy: {:.2f}%'.format(train_loss, train_acc))   \n",
    "        print('val Loss: {:.4f}, Accuracy: {:.2f}%'.format(val_loss, val_acc))\n",
    "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) \n",
    "    \n",
    "    # 정확도가 가장 높은 모델을 불러온 뒤 반환\n",
    "    model.load_state_dict(best_model_wts)  \n",
    "    return model\n",
    " \n",
    "# 베이스라인 모델을 정의한 함수를 이용해 학습\n",
    "base = train_baseline(model_base, train_loader, val_loader, optimizer, EPOCH)  #(16)\n",
    "\n",
    "# 학습 완료 모델 저장\n",
    "torch.save(base,'baseline.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "adf0af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform.compose()는 이미지 데이터의 전처리, Augmentation등의 과정에서 사용되는 메서드\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([transforms.Resize([64,64]), \n",
    "        # 좌우, 상하 반전, 괄호안에 p입력하면 비율 조정 가능. 기본값 0.5\n",
    "        transforms.RandomHorizontalFlip(),  \n",
    "                                 \n",
    "        # 이미지 일부를 랜덤하게 잘라 52*52사이즈로 변경, \n",
    "        #이미지 가운데 부분 혹은 오른쪽 위 또는 왼쪽 아래 부분만이 선택되게 할 수 있음 center crip 등 이용\n",
    "         transforms.ToTensor(), \n",
    "                                 \n",
    "        # 이미지가 tensor형태로 전환된 이후에 정규화를 시행, 정규화를 위해서는 평균과 표준편차 필요\n",
    "        # Normalize()메서드 내의 첫 번째 대괄호는 각각 R,G,B채널 값에서 정규화를 적용할 평균값의미\n",
    "        # 두번째 대괄호는 각각 정규화를 적용할 표준편차 값 의미\n",
    "        # 평균과 표준편차값은 Pre_trained_Model의 학습에 사용된 ImageNet의 값을 가져옴..\n",
    "        # 입력데이터의 정규화는 모델을 최적화하고, LocalMinimum에 빠지는 것을 방지하는 데 도움이 됨\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]),\n",
    "    \n",
    "    # 증강 제외하고 다 동일하게 가져옴.\n",
    "    'val': transforms.Compose([transforms.Resize([64,64]), transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "23c71ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './splitted2' \n",
    "\n",
    "# ImageFolder메서드는 데이터셋을 불러오는 메서드 위에서 학습함.\n",
    "# 딕셔너리 형태로 저장\n",
    "image_datasets = {x: ImageFolder(root=os.path.join(data_dir, x), transform=data_transforms[x]) for x in ['train', 'val']} \n",
    "\n",
    "# DataLoader메서드는 불러온 이미지 데이터를 주어진조건에 따라 미니배치 단위로 분리\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=4) for x in ['train', 'val']} \n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "# 클래스 이름 미리 저장\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ffc83b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 사전 학습 모델 불러오기\n",
    "from torchvision import models\n",
    "\n",
    "# pretrained = True로 하면 학습된 파라미터 값을 그대로 가져오고 False로 하면 모델의 구조만 가져오고 파라미터 랜덤.\n",
    "resnet = models.resnet50(pretrained=True)  \n",
    "\n",
    "resnet.eval()\n",
    "print(resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "41b5e869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 학습 모델 불러오기\n",
    "from torchvision import models\n",
    "\n",
    "# pretrained = True로 하면 학습된 파라미터 값을 그대로 가져오고 False로 하면 모델의 구조만 가져오고 파라미터 랜덤.\n",
    "resnet = models.resnet50(pretrained=True)  \n",
    "### 모델의 마지막 fully connected layer 대신 출력 채널의 수가 33개인 새로운 layer추가를 위해 마지막 레이어의 입력 채널 수 저장\n",
    "# in_fatures는 해당 layer의 입력 채널 수 의미\n",
    "num_ftrs = resnet.fc.in_features   \n",
    "# 마지막 레이어 fc를 새로운 레이어로 교체\n",
    "resnet.fc = nn.Linear(num_ftrs, 33) \n",
    "# 장비에 할당\n",
    "resnet = resnet.to(DEVICE)\n",
    " \n",
    "# 모델 학습시 사용하는 loss함수 저장\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "# 일부 레이어만 파라미터를 업데이트해야 하므로 filter()메서드와 lambda표현식을 사용하여 requires_grad = True로 설정된 Layer의 파라미터만 변경\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()), lr=0.001)\n",
    "\n",
    "# StepLR메서드를 이용하여 에포크에 따라 learninglate를 변경함. \n",
    "# step_size = 7, gamma = 0.1로 설정하면 7epoch마다 0.1씩 곱해 learningrate를 감소시킨다는 의미\n",
    "from torch.optim import lr_scheduler\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9ea0ad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 레이어가 몇번 째 레이어인지 나타내는 변수 ct\n",
    "\n",
    "ct = 0\n",
    "# 모델의 자식 모듈을 반복 가능한 객체로 반환하는 메서드.\n",
    "# resnet.children()은 생성한 resnet모델의 모든 layer정보를 담고 있음\n",
    "# 1~10까지의 레이어 (ResNet50에 존재하는) 중 상위레이어 6개를 파라미터 업데이트 하지않도록 해줌.\n",
    "for child in resnet.children():  \n",
    "    ct += 1  \n",
    "    if ct < 6: \n",
    "        # 각 레이어의 파라미터 텐서를 의미, 각텐서에는 requires_grad옵션이 있으며 기본값은 True\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2cd90c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_resnet(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())  \n",
    "    best_acc = 0.0  \n",
    "    \n",
    "    train_loss_list = []\n",
    "    train_accuracy_list = []\n",
    "    val_loss_list = []\n",
    "    val_accuracy_list = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('-------------- epoch {} ----------------'.format(epoch+1)) \n",
    "        since = time.time()                                     \n",
    "        for phase in ['train', 'val']: \n",
    "            if phase == 'train': \n",
    "                model.train() \n",
    "            else:\n",
    "                model.eval()     \n",
    " \n",
    "            running_loss = 0.0  \n",
    "            running_corrects = 0  \n",
    " \n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]: \n",
    "                inputs = inputs.to(DEVICE)  \n",
    "                labels = labels.to(DEVICE)  \n",
    "                \n",
    "                optimizer.zero_grad() \n",
    "                \n",
    "                # set_grad_enabled(phase == 'train')을 이용하여 train일 경우에만 업데이트 하도록 함.\n",
    "                with torch.set_grad_enabled(phase == 'train'):  \n",
    "                    outputs = model(inputs)  \n",
    "                    _, preds = torch.max(outputs, 1) \n",
    "                    loss = criterion(outputs, labels)  \n",
    "                    \n",
    "                    # train인 경우엔 가중치 업데이트\n",
    "                    if phase == 'train':   \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # 모든 데이터 Loss를 합산해서 저장하고자 \n",
    "                # 하나의 미니배치에 대해 계산된 Loss값에 데이터의 수를 곱해 합산\n",
    "                # input.size(0)은 Dataloader에서 전달되는 미니 배치의 데이터 수를 의미하는 것 = 배치 사이즈\n",
    "                running_loss += loss.item() * inputs.size(0)  \n",
    "                \n",
    "                # 맞으면 개수 1개 추가\n",
    "                running_corrects += torch.sum(preds == labels.data)  \n",
    "            \n",
    "            # 스케쥴러를 트레인일때만 가동\n",
    "            if phase == 'train':  \n",
    "                scheduler.step()\n",
    "                \n",
    "                # optimizer_ft.param_groups의 원소는 학습 과정에서의 파라미터를 저장하고 있는 딕셔너리\n",
    "                # 이중 학습률에 해당하는 key인 'lr'을 이용하여 각 에포크의 러닝 레이트를 가져옴\n",
    "                l_r = [x['lr'] for x in optimizer_ft.param_groups]\n",
    "                print('learning rate: ', l_r)\n",
    " \n",
    "            # 해당 에포크의 loss와 acc계산을 위해 미리 계산한 데이터셋 사이즈로 나눔.\n",
    "            epoch_loss = running_loss/dataset_sizes[phase]  \n",
    "            epoch_acc = running_corrects.double()/dataset_sizes[phase]  \n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_loss_list.append(epoch_loss)\n",
    "                train_accuracy_list.append(epoch_acc.to(torch.device(\"cpu\")))\n",
    "            elif phase == 'val': \n",
    "                val_loss_list.append(epoch_loss)\n",
    "                val_accuracy_list.append(epoch_acc.to(torch.device(\"cpu\")))\n",
    "            \n",
    " \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc)) \n",
    " \n",
    "            # 검증 단계에서 현재 Epoch의 정확도가 최고 정확도보다 높다면 best_acc를 현재 epoch의 정확도로 업데이트\n",
    "            if phase == 'val' and epoch_acc > best_acc: \n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    " \n",
    "        time_elapsed = time.time() - since  \n",
    "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    # 모델 로드 후 리턴\n",
    "    model.load_state_dict(best_model_wts) \n",
    "\n",
    "    return model, train_loss_list, train_accuracy_list, val_loss_list, val_accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4bb669ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- epoch 1 ----------------\n",
      "learning rate:  [0.001]\n",
      "train Loss: 1.7771 Acc: 0.3650\n",
      "val Loss: 6.4232 Acc: 0.1895\n",
      "Completed in 0m 6s\n",
      "-------------- epoch 2 ----------------\n",
      "learning rate:  [0.001]\n",
      "train Loss: 1.2822 Acc: 0.4752\n",
      "val Loss: 2.1886 Acc: 0.2157\n",
      "Completed in 0m 6s\n",
      "-------------- epoch 3 ----------------\n",
      "learning rate:  [0.001]\n",
      "train Loss: 1.1345 Acc: 0.5486\n",
      "val Loss: 2.5791 Acc: 0.3464\n",
      "Completed in 0m 6s\n",
      "-------------- epoch 4 ----------------\n",
      "learning rate:  [0.001]\n",
      "train Loss: 1.0538 Acc: 0.6069\n",
      "val Loss: 1.8914 Acc: 0.2157\n",
      "Completed in 0m 6s\n",
      "-------------- epoch 5 ----------------\n",
      "learning rate:  [0.001]\n",
      "train Loss: 0.9431 Acc: 0.6307\n",
      "val Loss: 2.7789 Acc: 0.3333\n",
      "Completed in 0m 6s\n",
      "-------------- epoch 6 ----------------\n",
      "learning rate:  [0.001]\n",
      "train Loss: 0.8469 Acc: 0.6760\n",
      "val Loss: 2.0854 Acc: 0.3595\n",
      "Completed in 0m 7s\n",
      "-------------- epoch 7 ----------------\n",
      "learning rate:  [0.0001]\n",
      "train Loss: 0.7489 Acc: 0.7192\n",
      "val Loss: 2.4807 Acc: 0.3464\n",
      "Completed in 0m 6s\n",
      "-------------- epoch 8 ----------------\n",
      "learning rate:  [0.0001]\n",
      "train Loss: 0.5210 Acc: 0.8186\n",
      "val Loss: 2.3051 Acc: 0.3137\n",
      "Completed in 0m 6s\n",
      "-------------- epoch 9 ----------------\n",
      "learning rate:  [0.0001]\n",
      "train Loss: 0.4360 Acc: 0.8575\n",
      "val Loss: 2.4466 Acc: 0.3464\n",
      "Completed in 0m 6s\n",
      "-------------- epoch 10 ----------------\n",
      "learning rate:  [0.0001]\n",
      "train Loss: 0.3319 Acc: 0.8726\n",
      "val Loss: 2.4571 Acc: 0.3791\n",
      "Completed in 0m 7s\n",
      "Best val Acc: 0.379085\n"
     ]
    }
   ],
   "source": [
    "model_resnet50, train_loss_list, train_accuracy_list, val_loss_list, val_accuracy_list = train_resnet(resnet, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=EPOCH) \n",
    "\n",
    "# 학습 완료 모델 저장\n",
    "torch.save(model_resnet50, 'resnet50.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293b6b37",
   "metadata": {},
   "source": [
    "### 토치에서 그래프 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0b959ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt, (tl, ta, vl, va) in enumerate(zip(train_loss_list, train_accuracy_list, val_loss_list, val_accuracy_list)):\n",
    "    train_accuracy_list[cnt] = ta.cpu()\n",
    "    val_accuracy_list[cnt] = va.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "36315ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.777091284292563,\n",
       "  1.282239049351241,\n",
       "  1.1344636799707268,\n",
       "  1.0537759856065458,\n",
       "  0.9431491660247868,\n",
       "  0.8468895137438763,\n",
       "  0.7489030603198716,\n",
       "  0.5209526084283,\n",
       "  0.4359613561250479,\n",
       "  0.3318537880483508],\n",
       " [tensor(0.3650, dtype=torch.float64),\n",
       "  tensor(0.4752, dtype=torch.float64),\n",
       "  tensor(0.5486, dtype=torch.float64),\n",
       "  tensor(0.6069, dtype=torch.float64),\n",
       "  tensor(0.6307, dtype=torch.float64),\n",
       "  tensor(0.6760, dtype=torch.float64),\n",
       "  tensor(0.7192, dtype=torch.float64),\n",
       "  tensor(0.8186, dtype=torch.float64),\n",
       "  tensor(0.8575, dtype=torch.float64),\n",
       "  tensor(0.8726, dtype=torch.float64)],\n",
       " [6.423187181061389,\n",
       "  2.1886072439305924,\n",
       "  2.5790654654596366,\n",
       "  1.8914137447581572,\n",
       "  2.7789421034794226,\n",
       "  2.0853809281891467,\n",
       "  2.480725984947354,\n",
       "  2.3051490269455255,\n",
       "  2.446586234896791,\n",
       "  2.4571466212179147],\n",
       " [tensor(0.1895, dtype=torch.float64),\n",
       "  tensor(0.2157, dtype=torch.float64),\n",
       "  tensor(0.3464, dtype=torch.float64),\n",
       "  tensor(0.2157, dtype=torch.float64),\n",
       "  tensor(0.3333, dtype=torch.float64),\n",
       "  tensor(0.3595, dtype=torch.float64),\n",
       "  tensor(0.3464, dtype=torch.float64),\n",
       "  tensor(0.3137, dtype=torch.float64),\n",
       "  tensor(0.3464, dtype=torch.float64),\n",
       "  tensor(0.3791, dtype=torch.float64)])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_list, train_accuracy_list, val_loss_list, val_accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "da4d54e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(train_loss_list,'--',c='red')\n",
    "plt.plot(val_loss_list,'--',c='blue')\n",
    "plt.plot(train_accuracy_list,'-',c='red')\n",
    "plt.plot(val_accuracy_list,'-',c='blue')\n",
    "plt.legend(['train_loss','train_accuracy', 'val_loss','val_accuracy'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1d5693ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가에 사용될 데이터를 확습에 사용된 데이터와 같은 방식으로 전처리\n",
    "# 먼저 베이스라인\n",
    "transform_base = transforms.Compose([transforms.Resize([64,64]),transforms.ToTensor()])\n",
    "test_base = ImageFolder(root='./splitted2/test',transform=transform_base)  \n",
    "test_loader_base = torch.utils.data.DataLoader(test_base, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d438ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전이학습에 필요한 전처리\n",
    "transform_resNet = transforms.Compose([\n",
    "        transforms.Resize([64,64]),  \n",
    "        transforms.RandomCrop(52),  \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "    ])\n",
    "    \n",
    "test_resNet = ImageFolder(root='./splitted2/test', transform=transform_resNet) \n",
    "test_loader_resNet = torch.utils.data.DataLoader(test_resNet, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fb0a1902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline test acc:   32.6797385620915\n"
     ]
    }
   ],
   "source": [
    "#베이스라인 모델 성능 평가하기\n",
    "baseline=torch.load('baseline.pt')\n",
    "\n",
    "# baseline.to(torch.device(\"cpu\"))\n",
    "# 모델을 평가모드로 전환\n",
    "baseline.eval()  \n",
    "\n",
    "# evaluate()함수를 이용하여 테스트 데이터에 대한 정확도 측정\n",
    "test_loss, test_accuracy = evaluate(baseline, test_loader_base)\n",
    "\n",
    "print('baseline test acc:  ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ec9fcdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet test acc:   14.379084967320262\n"
     ]
    }
   ],
   "source": [
    "#Transfer Learning 모델 성능 평가하기\n",
    "resnet50=torch.load('resnet50.pt') \n",
    "\n",
    "# 모델을 평가모드로 전환\n",
    "resnet50.eval()  \n",
    "\n",
    "# evaluate()함수를 이용하여 테스트 데이터에 대한 정확도 측정\n",
    "test_loss, test_accuracy = evaluate(resnet50, test_loader_resNet)\n",
    "\n",
    "print('ResNet test acc:  ', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e86637",
   "metadata": {},
   "source": [
    "## 새로운 이미지로 테스트(인터넷에서 다운 받은 이미지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b087f600",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_resNet = ImageFolder(root='./newImage', transform=transform_resNet) \n",
    "new_loader_resNet = torch.utils.data.DataLoader(new_resNet, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a965e390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 2\n",
       "    Root location: ./newImage\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=[64, 64], interpolation=bilinear, max_size=None, antialias=None)\n",
       "               RandomCrop(size=(52, 52), padding=None)\n",
       "               ToTensor()\n",
       "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "           )"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_resNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5024dc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet new acc:   50.0\n"
     ]
    }
   ],
   "source": [
    "#Transfer Learning 모델 성능 평가하기\n",
    "resnet50=torch.load('resnet50.pt') \n",
    "\n",
    "# 모델을 평가모드로 전환\n",
    "resnet50.eval()  \n",
    "\n",
    "# evaluate()함수를 이용하여 테스트 데이터에 대한 정확도 측정\n",
    "new_loss, new_accuracy = evaluate(resnet50, new_loader_resNet)\n",
    "\n",
    "print('ResNet new acc:  ', new_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad1aa89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
